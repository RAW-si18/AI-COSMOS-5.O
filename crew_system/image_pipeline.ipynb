{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from transformers import ViTFeatureExtractor, AutoTokenizer, VisionEncoderDecoderModel\n",
    "import onnxruntime as ort\n",
    "from paddleocr import PaddleOCR\n",
    "\n",
    "class ImageProcessingPipeline:\n",
    "    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.device = device\n",
    "        self.preprocess = self._init_preprocessing()\n",
    "        self.object_detection = self._init_object_detection()\n",
    "        self.classification = self._init_classification()\n",
    "        self.ocr = self._init_ocr()\n",
    "        self.captioning = self._init_captioning()\n",
    "\n",
    "    def _init_preprocessing(self):\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "    def _init_object_detection(self):\n",
    "        # Initialize YOLOv5-nano using ONNX Runtime for inference\n",
    "        model = ort.InferenceSession(\"yolov5n.onnx\")\n",
    "        return model\n",
    "\n",
    "    def _init_classification(self):\n",
    "        # Load quantized MobileNetV3-Small\n",
    "        model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v3_small', pretrained=True)\n",
    "        model.eval()\n",
    "        model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n",
    "        return model.to(self.device)\n",
    "\n",
    "    def _init_ocr(self):\n",
    "        # Initialize PaddleOCR with a lightweight model\n",
    "        return PaddleOCR(use_angle_cls=False, lang='en', use_gpu=self.device=='cuda')\n",
    "\n",
    "    def _init_captioning(self):\n",
    "        # Initialize ViT-GPT2 for image captioning\n",
    "        model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "        feature_extractor = ViTFeatureExtractor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "        return model.to(self.device), feature_extractor, tokenizer\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def process_image(self, image_path):\n",
    "        # Load and preprocess image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        input_tensor = self.preprocess(image).unsqueeze(0).to(self.device)\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        # Object Detection\n",
    "        od_input = input_tensor.cpu().numpy()\n",
    "        od_outputs = self.object_detection.run(None, {\"images\": od_input})\n",
    "        results['object_detection'] = od_outputs[0]\n",
    "\n",
    "        # Image Classification\n",
    "        class_output = self.classification(input_tensor)\n",
    "        results['classification'] = torch.argmax(class_output, dim=1).item()\n",
    "\n",
    "        # OCR\n",
    "        ocr_result = self.ocr.ocr(image_path, cls=False)\n",
    "        results['ocr'] = [line[1][0] for line in ocr_result]\n",
    "\n",
    "        # Image Captioning\n",
    "        model, feature_extractor, tokenizer = self.captioning\n",
    "        pixel_values = feature_extractor(images=image, return_tensors=\"pt\").pixel_values.to(self.device)\n",
    "        output_ids = model.generate(pixel_values, max_length=16, num_beams=4)\n",
    "        preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "        results['caption'] = preds[0].strip()\n",
    "\n",
    "        return results\n",
    "\n",
    "# Usage\n",
    "pipeline = ImageProcessingPipeline()\n",
    "results = pipeline.process_image('path_to_your_image.jpg')\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
