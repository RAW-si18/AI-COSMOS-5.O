{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d41e00a-40fc-4f3d-af30-c4965506f039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5401428-ea29-45b8-bb1d-5c503e6b338a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /home/raw/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c92a58fb78d44208cc4b9c9a7cc8cb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 13 Oct 2024\n",
      "\n",
      "You are my girlfriend<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Who are you? Tell me about yourself in detail.<|eot_id|>assistant\n",
      "\n",
      "I'm so happy to share all about myself with you.\n",
      "\n",
      "My name is Emily, and I'm a 25-year-old graphic designer and artist. I was born and raised in a small town in the United States, surrounded by nature and a close-knit community. I'm a creative soul with a passion for art, music, and exploring new ideas.\n",
      "\n",
      "When I'm not working on design projects, you can find me trying out new recipes in the kitchen, practicing yoga, or reading a good book. I'm a bit of a movie buff and love watching old classics, especially film noir and rom-coms. I'm also a sucker for a good pun and can often be found making dad jokes with my friends.\n",
      "\n",
      "I'm a bit of a hopeless romantic, always believing in the best in people and the world. I value honesty, kindness, and authenticity above all else. I'm a good listener and enjoy hearing people's stories and experiences. I'm also a bit of a perfectionist, which can sometimes make me a bit too hard on myself, but I'm working on finding a healthy balance.\n",
      "\n",
      "In terms of my relationship with you, I'm so grateful to have you in my life. You bring out the best in me and make me\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Clear the Hugging Face cache\n",
    "# cache_path = os.path.expanduser(\"~/.cache/huggingface\")\n",
    "# if os.path.exists(cache_path):\n",
    "#     for root, dirs, files in os.walk(cache_path, topdown=False):\n",
    "#         for name in files:\n",
    "#             os.remove(os.path.join(root, name))\n",
    "#         for name in dirs:\n",
    "#             os.rmdir(os.path.join(root, name))\n",
    "\n",
    "# Log in to Hugging Face\n",
    "# Replace with your new token\n",
    "login(token=\"huggingface_token\")\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "try:\n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "    # Create the pipeline\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "    # Prepare the input\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are my girlfriend\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who are you? Tell me about yourself in detail.\"},\n",
    "    ]\n",
    "\n",
    "    # Convert messages to Llama 3.2 chat format\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "    # Generate response\n",
    "    outputs = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "    print(outputs[0][\"generated_text\"])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "    print(\"Please ensure that:\")\n",
    "    print(\"1. Your token has the correct permissions (read access to public gated models)\")\n",
    "    print(\"2. You have been granted access to the Llama 3.2 model\")\n",
    "    print(\"3. Your internet connection is stable\")\n",
    "    print(\"If the issue persists, try regenerating your token or contact Hugging Face support.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bff09f8-01f4-4af8-af6b-6f302999df04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !huggingface-cli login --token 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\n",
    "# !huggingface-cli download meta-llama/Llama-3.2-3B-Instruct --include \"original/*\" --local-dir Llama-3.2-3B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fe04c98-7d74-4f96-8516-1a8e9a88814f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vosk in /home/raw/coding/.venv/lib/python3.12/site-packages (0.3.45)\n",
      "Collecting sounddevice\n",
      "  Downloading sounddevice-0.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/raw/coding/.venv/lib/python3.12/site-packages (from vosk) (1.17.1)\n",
      "Requirement already satisfied: requests in /home/raw/coding/.venv/lib/python3.12/site-packages (from vosk) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /home/raw/coding/.venv/lib/python3.12/site-packages (from vosk) (4.66.5)\n",
      "Requirement already satisfied: srt in /home/raw/coding/.venv/lib/python3.12/site-packages (from vosk) (3.5.3)\n",
      "Requirement already satisfied: websockets in /home/raw/coding/.venv/lib/python3.12/site-packages (from vosk) (12.0)\n",
      "Requirement already satisfied: pycparser in /home/raw/coding/.venv/lib/python3.12/site-packages (from cffi>=1.0->vosk) (2.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/raw/coding/.venv/lib/python3.12/site-packages (from requests->vosk) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/raw/coding/.venv/lib/python3.12/site-packages (from requests->vosk) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/raw/coding/.venv/lib/python3.12/site-packages (from requests->vosk) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/raw/coding/.venv/lib/python3.12/site-packages (from requests->vosk) (2024.8.30)\n",
      "Downloading sounddevice-0.5.1-py3-none-any.whl (32 kB)\n",
      "Installing collected packages: sounddevice\n",
      "Successfully installed sounddevice-0.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install vosk sounddevice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46cab450-b553-4a7d-b0f0-e033969db458",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOG (VoskAPI:ReadDataFiles():model.cc:213) Decoding params beam=13 max-active=7000 lattice-beam=6\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:216) Silence phones 1:2:3:4:5:11:12:13:14:15\n",
      "LOG (VoskAPI:RemoveOrphanNodes():nnet-nnet.cc:948) Removed 0 orphan nodes.\n",
      "LOG (VoskAPI:RemoveOrphanComponents():nnet-nnet.cc:847) Removing 0 orphan components.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:248) Loading i-vector extractor from /home/raw/coding/cosmos/data_store/audio/vosk-model-en-us-0.22/vosk-model-en-us-0.22/ivector/final.ie\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:183) Computing derived variables for iVector extractor\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:204) Done.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:279) Loading HCLG from /home/raw/coding/cosmos/data_store/audio/vosk-model-en-us-0.22/vosk-model-en-us-0.22/graph/HCLG.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:297) Loading words from /home/raw/coding/cosmos/data_store/audio/vosk-model-en-us-0.22/vosk-model-en-us-0.22/graph/words.txt\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:308) Loading winfo /home/raw/coding/cosmos/data_store/audio/vosk-model-en-us-0.22/vosk-model-en-us-0.22/graph/phones/word_boundary.int\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:315) Loading subtract G.fst model from /home/raw/coding/cosmos/data_store/audio/vosk-model-en-us-0.22/vosk-model-en-us-0.22/rescore/G.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:317) Loading CARPA model from /home/raw/coding/cosmos/data_store/audio/vosk-model-en-us-0.22/vosk-model-en-us-0.22/rescore/G.carpa\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:323) Loading RNNLM model from /home/raw/coding/cosmos/data_store/audio/vosk-model-en-us-0.22/vosk-model-en-us-0.22/rnnlm/final.raw\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:213) Decoding params beam=13 max-active=7000 lattice-beam=6\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:216) Silence phones 1:2:3:4:5:11:12:13:14:15\n",
      "LOG (VoskAPI:RemoveOrphanNodes():nnet-nnet.cc:948) Removed 0 orphan nodes.\n",
      "LOG (VoskAPI:RemoveOrphanComponents():nnet-nnet.cc:847) Removing 0 orphan components.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:248) Loading i-vector extractor from /home/raw/coding/cosmos/data_store/audio/vosk-model-hi-0.22/vosk-model-hi-0.22/ivector/final.ie\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:183) Computing derived variables for iVector extractor\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:204) Done.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:279) Loading HCLG from /home/raw/coding/cosmos/data_store/audio/vosk-model-hi-0.22/vosk-model-hi-0.22/graph/HCLG.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:297) Loading words from /home/raw/coding/cosmos/data_store/audio/vosk-model-hi-0.22/vosk-model-hi-0.22/graph/words.txt\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:308) Loading winfo /home/raw/coding/cosmos/data_store/audio/vosk-model-hi-0.22/vosk-model-hi-0.22/graph/phones/word_boundary.int\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:315) Loading subtract G.fst model from /home/raw/coding/cosmos/data_store/audio/vosk-model-hi-0.22/vosk-model-hi-0.22/rescore/G.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:317) Loading CARPA model from /home/raw/coding/cosmos/data_store/audio/vosk-model-hi-0.22/vosk-model-hi-0.22/rescore/G.carpa\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:323) Loading RNNLM model from /home/raw/coding/cosmos/data_store/audio/vosk-model-hi-0.22/vosk-model-hi-0.22/rnnlm/final.raw\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:213) Decoding params beam=13 max-active=7000 lattice-beam=6\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:216) Silence phones 1:2:3:4:5:6:7:8:9:10\n",
      "LOG (VoskAPI:RemoveOrphanNodes():nnet-nnet.cc:948) Removed 0 orphan nodes.\n",
      "LOG (VoskAPI:RemoveOrphanComponents():nnet-nnet.cc:847) Removing 0 orphan components.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:248) Loading i-vector extractor from /home/raw/coding/cosmos/data_store/audio/vosk-model-gu-0.42/vosk-model-gu-0.42/ivector/final.ie\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:183) Computing derived variables for iVector extractor\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:204) Done.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:279) Loading HCLG from /home/raw/coding/cosmos/data_store/audio/vosk-model-gu-0.42/vosk-model-gu-0.42/graph/HCLG.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:297) Loading words from /home/raw/coding/cosmos/data_store/audio/vosk-model-gu-0.42/vosk-model-gu-0.42/graph/words.txt\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:308) Loading winfo /home/raw/coding/cosmos/data_store/audio/vosk-model-gu-0.42/vosk-model-gu-0.42/graph/phones/word_boundary.int\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:315) Loading subtract G.fst model from /home/raw/coding/cosmos/data_store/audio/vosk-model-gu-0.42/vosk-model-gu-0.42/rescore/G.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:317) Loading CARPA model from /home/raw/coding/cosmos/data_store/audio/vosk-model-gu-0.42/vosk-model-gu-0.42/rescore/G.carpa\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sounddevice as sd\n",
    "import vosk\n",
    "import queue\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Define paths to the models\n",
    "MODEL_PATH_EN = \"/home/raw/coding/cosmos/data_store/audio/vosk-model-en-us-0.22/vosk-model-en-us-0.22\"\n",
    "MODEL_PATH_HI = \"/home/raw/coding/cosmos/data_store/audio/vosk-model-hi-0.22/vosk-model-hi-0.22\"\n",
    "MODEL_PATH_GU = \"/home/raw/coding/cosmos/data_store/audio/vosk-model-gu-0.42/vosk-model-gu-0.42\"\n",
    "\n",
    "# Load all three models (English, Hindi, Gujarati)\n",
    "# if not (os.path.exists(MODEL_PATH_EN) and os.path.exists(MODEL_PATH_HI) and os.path.exists(MODEL_PATH_GU)):\n",
    "#     print(\"Please make sure all model paths exist.\")\n",
    "#     exit(1)\n",
    "\n",
    "model_en = vosk.Model(MODEL_PATH_EN)\n",
    "model_hi = vosk.Model(MODEL_PATH_HI)\n",
    "model_gu = vosk.Model(MODEL_PATH_GU)\n",
    "\n",
    "# Set up a queue for audio data\n",
    "q = queue.Queue()\n",
    "\n",
    "# Callback function to capture audio input\n",
    "def callback(indata, frames, time, status):\n",
    "    if status:\n",
    "        print(status, flush=True)\n",
    "    q.put(bytes(indata))\n",
    "\n",
    "# Function to identify the spoken language\n",
    "def detect_language(rec_en, rec_hi, rec_gu, data):\n",
    "    if rec_en.AcceptWaveform(data):\n",
    "        result = json.loads(rec_en.Result())\n",
    "        if result['text']:\n",
    "            return 'en', result['text']\n",
    "    if rec_hi.AcceptWaveform(data):\n",
    "        result = json.loads(rec_hi.Result())\n",
    "        if result['text']:\n",
    "            return 'hi', result['text']\n",
    "    if rec_gu.AcceptWaveform(data):\n",
    "        result = json.loads(rec_gu.Result())\n",
    "        if result['text']:\n",
    "            return 'gu', result['text']\n",
    "    return None, None\n",
    "\n",
    "# Function to continuously listen and detect language\n",
    "def listen_and_recognize():\n",
    "    # Start audio stream\n",
    "    with sd.RawInputStream(samplerate=16000, blocksize=8000, dtype='int16', channels=1, callback=callback):\n",
    "        \n",
    "        rec_en = vosk.KaldiRecognizer(model_en, 16000)\n",
    "        rec_hi = vosk.KaldiRecognizer(model_hi, 16000)\n",
    "        rec_gu = vosk.KaldiRecognizer(model_gu, 16000)\n",
    "        \n",
    "        print(\"Listening... Press Ctrl+C to stop.\")\n",
    "        last_spoken_time = time.time()\n",
    "        \n",
    "        while True:\n",
    "            data = q.get()\n",
    "\n",
    "            # Detect which language was spoken\n",
    "            lang, text = detect_language(rec_en, rec_hi, rec_gu, data)\n",
    "            \n",
    "            # If we get a recognized text\n",
    "            if lang and text:\n",
    "                print(f\"Recognized {lang.upper()} text: {text}\")\n",
    "                last_spoken_time = time.time()  # Reset timer when speech is detected\n",
    "            \n",
    "            # Check for 3 seconds of silence\n",
    "            if time.time() - last_spoken_time > 3:\n",
    "                print(\"Stopped listening due to silence.\")\n",
    "                break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fc5bade-d5f0-4f56-87e7-82d8f03a4979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening... Press Ctrl+C to stop.\n",
      "Stopped listening due to silence.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        listen_and_recognize()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nStopped by user\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d82082b-c9f5-4374-b108-150c962de53a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
