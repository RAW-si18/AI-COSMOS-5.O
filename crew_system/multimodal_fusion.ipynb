{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Dict, List\n",
    "\n",
    "class MultimodalFusionLayer(nn.Module):\n",
    "    def __init__(self, image_dim=768, text_dim=512, fusion_dim=512):\n",
    "        super(MultimodalFusionLayer, self).__init__()\n",
    "        self.image_projection = nn.Linear(image_dim, fusion_dim)\n",
    "        self.text_projection = nn.Linear(text_dim, fusion_dim)\n",
    "        self.fusion = nn.MultiheadAttention(embed_dim=fusion_dim, num_heads=8)\n",
    "        self.norm = nn.LayerNorm(fusion_dim)\n",
    "        \n",
    "    def forward(self, image_features, text_features):\n",
    "        # Project image and text features to the same dimension\n",
    "        image_proj = self.image_projection(image_features).unsqueeze(0)  # [1, batch_size, fusion_dim]\n",
    "        text_proj = self.text_projection(text_features).unsqueeze(0)  # [1, batch_size, fusion_dim]\n",
    "        \n",
    "        # Concatenate image and text features\n",
    "        multimodal_features = torch.cat([image_proj, text_proj], dim=0)  # [2, batch_size, fusion_dim]\n",
    "        \n",
    "        # Apply multi-head attention\n",
    "        attn_output, _ = self.fusion(multimodal_features, multimodal_features, multimodal_features)\n",
    "        \n",
    "        # Add residual connection and normalize\n",
    "        fused_features = self.norm(multimodal_features + attn_output)\n",
    "        \n",
    "        return fused_features.mean(dim=0)  # [batch_size, fusion_dim]\n",
    "\n",
    "class TaskRoutingLayer(nn.Module):\n",
    "    def __init__(self, input_dim=512, num_tasks=7):\n",
    "        super(TaskRoutingLayer, self).__init__()\n",
    "        self.task_projection = nn.Linear(input_dim, num_tasks)\n",
    "        self.task_names = [\n",
    "            \"Summarization\",\n",
    "            \"Question_Answering\",\n",
    "            \"Code_Generation\",\n",
    "            \"Translation\",\n",
    "            \"Paraphrasing\",\n",
    "            \"Sentiment_Analysis\",\n",
    "            \"Grammar_Correction\"\n",
    "        ]\n",
    "        \n",
    "    def forward(self, fused_features):\n",
    "        task_scores = self.task_projection(fused_features)\n",
    "        task_probs = F.softmax(task_scores, dim=-1)\n",
    "        return task_probs\n",
    "\n",
    "class MultimodalProcessor:\n",
    "    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.device = device\n",
    "        self.fusion_layer = MultimodalFusionLayer().to(device)\n",
    "        self.routing_layer = TaskRoutingLayer().to(device)\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def process(self, image_output: Dict, text_output: Dict) -> Dict:\n",
    "        # Extract relevant features\n",
    "        image_features = torch.tensor(image_output['classification_features'], device=self.device)\n",
    "        ocr_embedding = text_output['ocr_processed']['embedding'].to(self.device)\n",
    "        caption_embedding = text_output['caption_processed']['embedding'].to(self.device)\n",
    "        \n",
    "        # Combine text embeddings (you might want to use a more sophisticated method)\n",
    "        text_features = (ocr_embedding + caption_embedding) / 2\n",
    "        \n",
    "        # Fuse multimodal features\n",
    "        fused_features = self.fusion_layer(image_features, text_features)\n",
    "        \n",
    "        # Route to tasks\n",
    "        task_probabilities = self.routing_layer(fused_features)\n",
    "        \n",
    "        # Prepare output\n",
    "        task_routing = {task: prob.item() for task, prob in zip(self.routing_layer.task_names, task_probabilities[0])}\n",
    "        \n",
    "        return {\n",
    "            'fused_features': fused_features.cpu().numpy(),\n",
    "            'task_routing': task_routing\n",
    "        }\n",
    "\n",
    "# Usage\n",
    "def main():\n",
    "    # Simulated outputs from previous pipelines\n",
    "    image_output = {\n",
    "        'object_detection': [[0, 0, 100, 100, 0.9, 1]],\n",
    "        'classification': 5,\n",
    "        'classification_features': [0.1] * 768,  # Simulated feature vector\n",
    "        'ocr': ['Hello', 'World'],\n",
    "        'caption': 'A computer screen displaying text'\n",
    "    }\n",
    "    \n",
    "    text_output = {\n",
    "        'ocr_processed': {\n",
    "            'preprocessed_text': 'hello world',\n",
    "            'language': 'en',\n",
    "            'embedding': torch.randn(512)\n",
    "        },\n",
    "        'caption_processed': {\n",
    "            'preprocessed_text': 'computer screen displaying text',\n",
    "            'language': 'en',\n",
    "            'embedding': torch.randn(512)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    processor = MultimodalProcessor()\n",
    "    result = processor.process(image_output, text_output)\n",
    "    \n",
    "    print(\"Fused Features Shape:\", result['fused_features'].shape)\n",
    "    print(\"\\nTask Routing Probabilities:\")\n",
    "    for task, prob in result['task_routing'].items():\n",
    "        print(f\"{task}: {prob:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
